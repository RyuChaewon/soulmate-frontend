// src/pages/RecordingPage.jsx
import { useEffect, useRef, useCallback, useState } from 'react';
import { useNavigate, useParams } from 'react-router-dom';
import { io } from 'socket.io-client';
import Layout from '../components/Layout/Layout';
import * as S from './RecordingPage.styles';

// ì•„ë°”íƒ€ ì´ë¯¸ì§€ ë‘ ê°€ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
import avatarIcon from '../assets/icons/avatar.svg';
import avatar2Icon from '../assets/icons/avatar_2.svg'; // ì•„ë°”íƒ€ê°€ ë§í•  ë•Œ ì‚¬ìš©í•  ì´ë¯¸ì§€
import endRecordingButtonImg from '../assets/buttons/endrecordingbutton.svg';

function RecordingPage() {
  const { date } = useParams();
  const navigate = useNavigate();

  const [userId] = useState(() => localStorage.getItem('uuid') || 'b23cbc0e-d22b-4ce4-8178-f936f87a19c9');
  
  const videoRef = useRef(null);
  const socketRef = useRef(null);
  const localStreamRef = useRef(null);
  const mediaRecorderRef = useRef(null);
  const streamIntervalRef = useRef(null);
  
  const sessionIdRef = useRef(null);
  const frameSequenceRef = useRef(0);
  const audioSequenceRef = useRef(0);

  const [isAvatarSpeaking, setIsAvatarSpeaking] = useState(false);
  const [currentAvatarImage, setCurrentAvatarImage] = useState(avatarIcon);
  const [aiResponseText, setAiResponseText] = useState('');

  const speak = useCallback((text) => {
    if (!window.speechSynthesis) return alert("ìŒì„± í•©ì„±ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.");
    
    window.speechSynthesis.cancel();
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'ko-KR';
    utterance.rate = 1.1;
    
    utterance.onstart = () => setIsAvatarSpeaking(true);
    utterance.onend = () => setIsAvatarSpeaking(false);
    
    console.log(`ğŸ¤ ì•„ë°”íƒ€ ìŒì„± ì¬ìƒ: "${text}"`);
    window.speechSynthesis.speak(utterance);
  }, []);

  // --- 3. [ìˆ˜ì •ë¨] ëª¨ë“  ìŠ¤íŠ¸ë¦¼ê³¼ ì—°ê²°ì„ ì¤‘ì§€í•˜ëŠ” í•¨ìˆ˜ ---
  const stopAllStreams = useCallback(() => {
    console.log('--- ğŸ›‘ ëª¨ë“  ìŠ¤íŠ¸ë¦¼ê³¼ ì—°ê²°ì„ ì¤‘ì§€í•©ë‹ˆë‹¤ ---');

    if (streamIntervalRef.current) clearInterval(streamIntervalRef.current);
    streamIntervalRef.current = null;
    frameSequenceRef.current = 0;

    if (localStreamRef.current) {
      localStreamRef.current.getTracks().forEach(track => track.stop());
      localStreamRef.current = null;
    }
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
    }
    mediaRecorderRef.current = null;
    audioSequenceRef.current = 0;

    if (videoRef.current) videoRef.current.srcObject = null;

    window.speechSynthesis.cancel();
    setIsAvatarSpeaking(false);
    setAiResponseText('');

    if (socketRef.current) {
      if (sessionIdRef.current) {
        console.log(`[Socket] 'stop-video-stream' ì´ë²¤íŠ¸ ì „ì†¡: sessionId=${sessionIdRef.current}, userId=${userId}`);
        
        // DTO ê·œê²©ì— ë§ê²Œ 'reason' í•„ë“œ í¬í•¨ (Optional)
        socketRef.current.emit('stop-video-stream', { 
          sessionId: sessionIdRef.current, 
          userId: userId,
          reason: 'ì‚¬ìš©ì ì¢…ë£Œ'
        });
      }
      
      // --- âœ… [í•µì‹¬ ìˆ˜ì •] ---
      // â—ï¸ emit() ì§í›„ disconnect()ë¥¼ í˜¸ì¶œí•˜ë©´ ì„œë²„ê°€ ë©”ì‹œì§€ë¥¼ ë°›ì§€ ëª»í•˜ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.
      // â—ï¸ ì—°ê²° ì¢…ë£ŒëŠ” useEffectì˜ cleanup í•¨ìˆ˜ ë˜ëŠ” ì„œë²„ì˜ handleDisconnectê°€ ì²˜ë¦¬í•©ë‹ˆë‹¤.
      // socketRef.current.disconnect(); 
      // socketRef.current = null;
    }
    sessionIdRef.current = null;
  }, [userId]); // userId ì˜ì¡´ì„± ì¶”ê°€

  // --- 4. ì˜ìƒ ìº¡ì³ (DTOì— ë§ì¶¤) ---
  const startFrameCapture = useCallback(() => {
    if (!localStreamRef.current || !socketRef.current || !videoRef.current) return;
    
    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d');
    canvas.width = 640; canvas.height = 480;
    
    const tempSessionId = 'session_' + Date.now();
    
    console.log(`[Socket] 'start-video-stream' ì´ë²¤íŠ¸ ì „ì†¡: sessionId=${tempSessionId}, userId=${userId}`);
    
    // DTO(StartVideoStreamDto) ê·œê²©ì— ë§ì¶¤
    socketRef.current.emit('start-video-stream', {
      sessionId: tempSessionId, 
      userId: userId,
      quality: {
        width: 640, 
        height: 480,
        frameRate: 30,
        bitrate: 1000
      },
      enableAudio: true,
      recordingEnabled: false,
      aiProcessingEnabled: true
    });
    
    streamIntervalRef.current = setInterval(() => {
      if (videoRef.current && videoRef.current.videoWidth > 0 && socketRef.current && sessionIdRef.current) {
        ctx.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);
        const frameData = canvas.toDataURL('image/jpeg', 0.4).split(',')[1];
        const frameId = 'frame_v_' + Date.now() + '_' + frameSequenceRef.current;
        
        // DTO(VideoFrameDto) ê·œê²©ì— ë§ì¶¤
        socketRef.current.emit('video-frame', { 
          sessionId: sessionIdRef.current,
          frameId: frameId,
          timestamp: Date.now(),
          frameData, 
          sequenceNumber: frameSequenceRef.current++
        });
      }
    }, 150);
    console.log('ğŸ“¹ ì˜ìƒ ìº¡ì²˜ ì‹œì‘');
  }, [userId]);

  // --- 5. ìŒì„± ë…¹ìŒ ì„¤ì • (DTOì— ë§ì¶¤) ---
  const setupAudioCapture = useCallback(() => {
    if (!localStreamRef.current || !socketRef.current) return;
    try {
      const audioTrack = localStreamRef.current.getAudioTracks()[0];
      if (!audioTrack) {
        console.error('âŒ ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
        return;
      }
      
      const audioOnlyStream = new MediaStream([audioTrack]);
      const mimeTypeToUse = 'audio/webm;codecs=opus';

      if (!MediaRecorder.isTypeSupported(mimeTypeToUse)) {
        console.error('âŒ audio/webm;codecs=opus íƒ€ì…ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
        return;
      }
      
      const recorder = new MediaRecorder(audioOnlyStream, {
        mimeType: mimeTypeToUse,
        audioBitsPerSecond: 128000
      });
      mediaRecorderRef.current = recorder;

      recorder.ondataavailable = (event) => {
        if (event.data.size > 100) {
          const reader = new FileReader();
          reader.onloadend = () => {
            if (socketRef.current && socketRef.current.connected && sessionIdRef.current) {
              const fullDataUrl = reader.result;
              const base64Prefix = 'base64,';
              const prefixIndex = fullDataUrl.indexOf(base64Prefix);
              let base64Audio = '';

              if (prefixIndex > -1) {
                base64Audio = fullDataUrl.substring(prefixIndex + base64Prefix.length);
              } else {
                base64Audio = fullDataUrl;
              }
              
              const audioFrameId = 'audio_' + Date.now() + '_' + audioSequenceRef.current;
              
              // DTO(AudioFrameDto) ê·œê²©ì— ë§ì¶¤
              socketRef.current.emit('audio-frame', {
                sessionId: sessionIdRef.current,
                frameId: audioFrameId,
                timestamp: Date.now(),
                audioData: base64Audio,
                sequenceNumber: audioSequenceRef.current++,
                format: 'webm'
              });
            }
          };
          reader.readAsDataURL(event.data);
        }
        
        if (mediaRecorderRef.current) {
           mediaRecorderRef.current.stop();
        }
      };

      recorder.onstop = () => {
        if (localStreamRef.current && mediaRecorderRef.current) { 
          mediaRecorderRef.current.start(5000);
        } else {
          console.log('ğŸ¤ ë…¹ìŒ ë£¨í”„ ì •ì§€ (ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¨)');
        }
      };

      recorder.start(5000);
      console.log('ğŸ¤ ìŒì„± ë…¹ìŒ ì‹œì‘ (5ì´ˆ ê°„ê²©)');
    } catch (error) {
      console.error(`âŒ ìŒì„± ìº¡ì²˜ ì‹¤íŒ¨: ${error.message}`);
    }
  }, []);

  // --- 6. ì•„ë°”íƒ€ ë§í•˜ê¸° ì• ë‹ˆë©”ì´ì…˜ useEffect ---
  useEffect(() => {
    let animationInterval = null;
    
    if (isAvatarSpeaking) {
      animationInterval = setInterval(() => {
        setCurrentAvatarImage(prev => (prev === avatarIcon ? avatar2Icon : avatarIcon));
      }, 300);
    } else {
      setCurrentAvatarImage(avatarIcon);
    }

    return () => {
      if (animationInterval) {
        clearInterval(animationInterval);
      }
    };
  }, [isAvatarSpeaking]);

  // --- 7. í˜ì´ì§€ ë¡œë“œ useEffect (STT ìˆ˜ì •) ---
  useEffect(() => {
    const unlockAudioContext = () => {
      const voices = window.speechSynthesis.getVoices(); 
      const speakDummy = () => {
        if (window.speechSynthesis.speaking || window.speechSynthesis.pending) return;
        const dummyUtterance = new SpeechSynthesisUtterance(' ');
        dummyUtterance.volume = 0;
        dummyUtterance.lang = 'ko-KR';
        window.speechSynthesis.speak(dummyUtterance);
      };
      if (voices.length > 0) speakDummy();
      else window.speechSynthesis.onvoiceschanged = () => speakDummy();
    };

    const startProcess = async () => {
      socketRef.current = io('https://soulmate.kro.kr/video', {
        transports: ['websocket', 'polling'],
        secure: true
      });

      socketRef.current.on('connect', () => console.log('âœ… ì„œë²„ ì—°ê²°ë¨'));
      socketRef.current.on('disconnect', () => console.log('âŒ ì„œë²„ ì—°ê²° í•´ì œë¨'));
      
      socketRef.current.on('video-stream-started', (data) => {
        console.log('ğŸ‰ [ON] video-stream-started', data);
        if(data.sessionId) sessionIdRef.current = data.sessionId;
      });

      let diaryTextBuffer = '';
      socketRef.current.on('diary-stream-start', () => {
        diaryTextBuffer = '';
        setAiResponseText('');
      });

      socketRef.current.on('diary-stream-chunk', (chunk) => {
        diaryTextBuffer += chunk;
        setAiResponseText(prev => prev + chunk);
      });

      socketRef.current.on('diary-stream-end', () => {
        if (diaryTextBuffer.trim()) speak(diaryTextBuffer);
        diaryTextBuffer = '';
      });
      
      socketRef.current.on('diary-analysis', (data) => {
        console.log('ğŸ§  [ON] diary-analysis', data);
        if (data && data.advice) {
          speak(data.advice);
          setAiResponseText(data.advice);
        }
      });
      
      socketRef.current.on('stt-transcript', (data) => {
        console.log(`ğŸ—£ï¸ [ON] stt-transcript: ${data.text}`);
      });
      
      socketRef.current.on('final-diary', (data) => {
         console.log('ğŸ“— [ON] final-diary (ì¼ê¸° ìƒì„± ì™„ë£Œ ì‹ í˜¸)', data);
         // (ì°¸ê³ ) UIë¥¼ ë°”ê¾¸ì§€ ì•Šê¸° ìœ„í•´ ì´ ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ,
         // ì„œë²„ê°€ ì™„ë£Œí–ˆìŒì„ í™•ì¸í•˜ëŠ” ë¡œê·¸ì…ë‹ˆë‹¤.
      });

      socketRef.current.on('video-stream-error', (err) => console.error('âŒ [ON] video-stream-error', err));
      socketRef.current.on('diary-stream-error', (err) => console.error('âŒ [ON] diary-stream-error', err));
      socketRef.current.on('exception', (err) => console.error('âŒ [ON] exception (ì„œë²„ ì˜¤ë¥˜)', err));

      try {
        // --- âœ… [í•µì‹¬ ìˆ˜ì •] ---
        const stream = await navigator.mediaDevices.getUserMedia({ 
          video: { width: 640, height: 480 },
          audio: {
            sampleRate: 48000, // â—ï¸ Google STT Opus ì½”ë±ì´ ì§€ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ì§€ì •
            channelCount: 1,  // â—ï¸ ëª¨ë…¸ ì±„ë„ë¡œ ì§€ì • (ê¶Œì¥)
          }
        });

        const audioSettings = stream.getAudioTracks()[0].getSettings();
        console.log('ğŸ¤ ì‹¤ì œ ì ìš©ëœ ì˜¤ë””ì˜¤ ì„¤ì •:', audioSettings);

        unlockAudioContext();
        localStreamRef.current = stream;
        if (videoRef.current) videoRef.current.srcObject = stream;
        startFrameCapture();
        setupAudioCapture();
      } catch (error) {
        console.error('âŒ ë¯¸ë””ì–´ ì ‘ê·¼ ì‹¤íŒ¨:', error);
        alert('ì¹´ë©”ë¼/ë§ˆì´í¬ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
      }
    };
    
    startProcess();

    return () => {
      // ì»´í¬ë„ŒíŠ¸ê°€ ì‚¬ë¼ì§ˆ ë•Œ (í˜ì´ì§€ ì´ë™ ì‹œ) ì‹¤í–‰ë˜ëŠ” ì •ë¦¬ í•¨ìˆ˜
      stopAllStreams();
      if (socketRef.current) {
        socketRef.current.disconnect();
        socketRef.current = null;
      }
    };
  }, [date, navigate, speak, setupAudioCapture, startFrameCapture, stopAllStreams, userId]);

  // --- 8. 'ê¸°ë¡ ë' ë²„íŠ¼ í•¸ë“¤ëŸ¬ (UI ë³€ê²½ ì—†ìŒ) ---
  const handleEndRecording = () => {
    stopAllStreams(); // 1. ì„œë²„ì— "ì¼ê¸° ìƒì„±" ìš”ì²­ (ë²„ê·¸ ìˆ˜ì •ë¨)
    navigate(`/after-record/${date}`); // 2. (ìš”ì²­ëŒ€ë¡œ) ì¦‰ì‹œ ì´ë™
  };

  return (
    <Layout>
      <S.ContentContainer>
        <S.UserVideoWrapper>
          <S.LiveVideo ref={videoRef} autoPlay playsInline muted />
        </S.UserVideoWrapper>
        
        <S.AvatarVideoWrapper>
          {aiResponseText && (
            <S.AvatarSpeechBubble>
              {aiResponseText}
            </S.AvatarSpeechBubble>
          )}
          <S.AvatarImage src={currentAvatarImage} alt="ì•„ë°”íƒ€" />
        </S.AvatarVideoWrapper>
        
        <S.EndRecordingButtonWrapper onClick={handleEndRecording}>
          <img src={endRecordingButtonImg} alt="ê¸°ë¡ ë" />
        </S.EndRecordingButtonWrapper>
      </S.ContentContainer>
    </Layout>
  );
}

export default RecordingPage;